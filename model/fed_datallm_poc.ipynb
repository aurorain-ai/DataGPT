{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Federated Data LLMs with the centralized incremental vector store to fine-tune distributed GPTs**\n\nFederation relationship:\n1. Centralized vector store (sharing data) and GPT-4 (knowledge)\n2. Edges: fine-tuned GPT-J and enterprise data warehouses\n\nFederation logic and algorithms:\n1. The vector store contains both private data (like data catalog) and shareable data (like filtered operational data)\n2. The LLM vectors from prompts to GPT-4\n3. Fine-tune GPT-J with vector data\n\nFine-tune GPT-J specifically in three areas:\n1. Froze model with low-rank adapters (LoRA) with 8-bit backbone\n2. Train from knowledge checkpoint (for globally sharing)\n3. Train from vector data checkpoint (for incremental edge)\n\n\nThis notebook is a proof of concept for fine-tuning GPT-J only due to external connectors and resource limitations\n\nGPT-J fine-tuning References:\n* In March 2023, Databricks released Dolly, an Apache-licensed, instruction-following model based on GPT-J with fine-tuning from the Stanford Alpaca dataset: https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html\n* Fine-tuning 6B GPT-J in Colab: https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es\n* Fine-tuning GPT-J-6B in Github thread: https://github.com/huggingface/transformers/issues/14839\n* GPT-J-6B Fine-Tuning with Ray AIR: https://docs.ray.io/en/latest/ray-air/examples/gptj_deepspeed_fine_tuning.html","metadata":{}},{"cell_type":"code","source":"# Comment out for installing once\n\n#! pip install \"datasets\" \"evaluate\" \"accelerate>=0.16.0\" \"transformers>=4.26.0\" \"torch>=1.12.0\" \"deepspeed\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.cuda.amp import custom_fwd, custom_bwd\n\nfrom bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n\nfrom tqdm.auto import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FrozenBNBLinear(nn.Module):\n    def __init__(self, weight, absmax, code, bias=None):\n        assert isinstance(bias, nn.Parameter) or bias is None\n        super().__init__()\n        self.out_features, self.in_features = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n        self.bias = bias\n \n    def forward(self, input):\n        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n \n    @classmethod\n    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n        return cls(weights_int8, *state, linear.bias)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n \n \nclass DequantizeAndLinear(torch.autograd.Function): \n    @staticmethod\n    @custom_fwd\n    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        ctx.save_for_backward(input, weights_quantized, absmax, code)\n        ctx._has_bias = bias is not None\n        return F.linear(input, weights_deq, bias)\n \n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output: torch.Tensor):\n        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n        input, weights_quantized, absmax, code = ctx.saved_tensors\n        # grad_output: [*batch, out_features]\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        grad_input = grad_output @ weights_deq\n        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n        return grad_input, None, None, None, grad_bias\n \n \nclass FrozenBNBEmbedding(nn.Module):\n    def __init__(self, weight, absmax, code):\n        super().__init__()\n        self.num_embeddings, self.embedding_dim = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n \n    def forward(self, input, **kwargs):\n        with torch.no_grad():\n            # note: both quantuized weights and input indices are *not* differentiable\n            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n            output = F.embedding(input, weight_deq, **kwargs)\n        if self.adapter:\n            output += self.adapter(input)\n        return output \n \n    @classmethod\n    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n        return cls(weights_int8, *state)\n \n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n \n \ndef quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n    assert chunk_size % 4096 == 0\n    code = None\n    chunks = []\n    absmaxes = []\n    flat_tensor = matrix.view(-1)\n    for i in range((matrix.numel() - 1) // chunk_size + 1):\n        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n        chunks.append(quantized_chunk)\n        absmaxes.append(absmax_chunk)\n \n    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n    absmax = torch.cat(absmaxes)\n    return matrix_i8, (absmax, code)\n \n \ndef convert_to_int8(model):\n    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n    for module in list(model.modules()):\n        for name, child in module.named_children():\n            if isinstance(child, nn.Linear):\n                print(name, child)\n                setattr( \n                    module,\n                    name,\n                    FrozenBNBLinear(\n                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                        bias=child.bias,\n                    ),\n                )\n            elif isinstance(child, nn.Embedding):\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBEmbedding(\n                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                    )\n                )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n    def __init__(self, config):\n        super().__init__(config)\n\n        convert_to_int8(self.attn)\n        convert_to_int8(self.mlp)\n\n\nclass GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n        \n\nclass GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\ntransformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ngpt.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = tokenizer(\"A cat sat on a mat\", return_tensors='pt')\nprompt = {key: value.to(device) for key, value in prompt.items()}\nout = gpt.generate(**prompt, min_length=128, max_length=128, do_sample=True)\ntokenizer.decode(out[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LoRA fine-tuning example\n\nHere we demonstrate how to fine-tune the proposed model using low-rank adapters (Hu et al, 2021) and 8-bit Adam. We also use dataset streaming API to avoid downloading the large dataset.","metadata":{}},{"cell_type":"code","source":"def add_adapters(model, adapter_dim=16):\n    assert adapter_dim > 0\n\n    for module in model.modules():\n        if isinstance(module, FrozenBNBLinear):\n            module.adapter = nn.Sequential(\n                nn.Linear(module.in_features, adapter_dim, bias=False),\n                nn.Linear(adapter_dim, module.out_features, bias=False),\n            )\n            nn.init.zeros_(module.adapter[1].weight)\n        elif isinstance(module, FrozenBNBEmbedding):\n            module.adapter = nn.Sequential(\n                nn.Embedding(module.num_embeddings, adapter_dim),\n                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n            )\n            nn.init.zeros_(module.adapter[1].weight)\n\nadd_adapters(gpt)\ngpt.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom bitsandbytes.optim import Adam8bit\n\ngpt.gradient_checkpointing_enable()\n\ncodeparrot = load_dataset(\"transformersbook/codeparrot-train\", streaming=True)\noptimizer = Adam8bit(gpt.parameters(), lr=1e-5)\n\nwith torch.cuda.amp.autocast():\n    for row in tqdm(codeparrot[\"train\"]):\n        if len(row[\"content\"]) <= 1:\n            continue\n\n        batch = tokenizer(row[\"content\"], truncation=True, max_length=128, return_tensors='pt')\n        batch = {k: v.cuda() for k, v in batch.items()}\n\n        out = gpt.forward(**batch,)\n\n        loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n                               reduction='mean')\n        print(loss)\n        loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()","metadata":{},"execution_count":null,"outputs":[]}]}